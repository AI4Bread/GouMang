
# XiXiLM 

<div align="center">

<img src="https://github.com/AI4Bread/GouMang/blob/main/assets/goumang_logoallnew.png?raw=true" width="600"/>
  <div>&nbsp;</div>
  <div align="center">
<!--     <b><font size="5">XiXiLM</font></b> -->
    <sup>
      <a href="http://www.ai4bread.com">
      </a>
    </sup>
    <div>&nbsp;</div>
  </div>
  

[ğŸ’»Github Repo](https://github.com/AI4Bread/GouMang) â€¢ [ğŸ¤”Reporting Issues](https://github.com/AI4Bread/GouMang/issues) â€¢ [ğŸ“œTechnical Report](https://github.com/AI4Bread)

</div>

<p align="center">
    ğŸ‘‹ join us on <a href="https://github.com/AI4Bread/GouMang" target="_blank">Github</a>
</p>



## Introduction

XiXiLMï¼ˆGouMang LLMï¼‰ has open-sourced a 7 billion parameter base model and a chat model tailored for agricultural scenarios. The model has the following characteristics:

- **200K Context window**: Nearly perfect at finding needles in the haystack with 200K-long context, with leading performance on long-context tasks like LongBench and L-Eval. Try it with [LMDeploy](https://github.com/InternLM/lmdeploy) for 200K-context inference.

- **Outstanding comprehensive performance**: Significantly better than the last generation in all dimensions, especially in reasoning, math, code, chat experience, instruction following, and creative writing, with leading performance among open-source models in similar sizes. In some evaluations, InternLM2-Chat-20B may match or even surpass ChatGPT (GPT-3.5).

## XiXiLM-Qwen-14B


**Limitations:** Although we have made efforts to ensure the safety of the model during the training process and to 
encourage the model to generate text that complies with ethical and legal requirements, the model may still produce unexpected 
outputs due to its size and probabilistic generation paradigm. For example, the generated responses may contain biases, discrimination, 
or other harmful content. Please do not propagate such content. We are not responsible for any consequences resulting from the 
dissemination of harmful information.

### Import from Transformers

To load the XiXiLM model using Transformers, use the following code:

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained("AI4Bread/XiXi_Qwen_base_14b", trust_remote_code=True)
# Set `torch_dtype=torch.float16` to load model in float16, otherwise it will be loaded as float32 and cause OOM Error.
model = AutoModelForCausalLM.from_pretrained("AI4Bread/XiXi_Qwen_base_14b", torch_dtype=torch.float16, trust_remote_code=True).cuda()
model = model.eval()
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
# Hello! How can I help you today?
response, history = model.chat(tokenizer, "é©¬é“ƒè–¯è‚²ç§æœ‰ä»€ä¹ˆæ³¨æ„äº‹é¡¹ï¼Ÿéœ€è¦æ³¨æ„ä»€ä¹ˆå‘¢ï¼Ÿ", history=history)
print(response)
```

The responses can be streamed using `stream_chat`:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = "AI4Bread/XiXi_Qwen_base_14b"
model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, trust_remote_code=True).cuda()
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

model = model.eval()
length = 0
for response, history in model.stream_chat(tokenizer, "Hello", history=[]):
    print(response[length:], flush=True, end="")
    length = len(response)
```


## Deployment

### LMDeploy

LMDeploy is a toolkit for compressing, deploying, and serving LLM, developed by the MMRazor and MMDeploy teams.

```bash
pip install lmdeploy
```

Or you can launch an OpenAI compatible server with the following command:

```bash
lmdeploy serve api_server internlm/internlm2-chat-7b --model-name internlm2-chat-7b --server-port 23333 
```

Then you can send a chat request to the server:

```bash
curl http://localhost:23333/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
    "model": "internlm2-chat-7b",
    "messages": [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†œä¸šä¸“å®¶"},
    {"role": "user", "content": "é©¬é“ƒè–¯ç§æ¤çš„æ—¶å€™æœ‰å“ªäº›æ³¨æ„äº‹é¡¹ï¼Ÿ"}
    ]
    }'
```

The output be like:

![image/png](https://cdn-uploads.huggingface.co/production/uploads/658a3c4cbbb04840e3ce7e2c/NPdRr5Y5l5E0m0URCVZ1f.png)



Find more details in the [LMDeploy documentation](https://lmdeploy.readthedocs.io/en/latest/)

### vLLM

Launch OpenAI compatible server with `vLLM>=0.3.2`:

```bash
pip install vllm
```

```bash
python -m vllm.entrypoints.openai.api_server --model internlm/internlm2-chat-7b --served-model-name internlm2-chat-7b --trust-remote-code
```

Then you can send a chat request to the server:

```bash
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
    "model": "internlm2-chat-7b",
    "messages": [
    {"role": "system", "content": "You are a professional agriculture expert."},
    {"role": "user", "content": "Introduce potato farming to me."}
    ]
    }'
```

Find more details in the [vLLM documentation](https://docs.vllm.ai/en/latest/index.html)

### Convert  lmdeploy TurboMind

```bash
# Converting Model to TurboMind (FastTransformer Format)
lmdeploy convert internlm-chat-7b /path/to/XiXiLM
```

Here, we will use our pre-trained model file and execute the conversion in the user's root directory, as shown below.

```bash
lmdeploy convert internlm2-chat-7b /root/autodl-tmp/agri_intern/XiXiLM --tokenizer-path ./GouMang/tokenizer.json
```

After execution, a workspace folder will be generated in the current directory. 
This folder contains the necessary files for TurboMind and Triton "Model Inference." as shown below:


![image/png](https://cdn-uploads.huggingface.co/production/uploads/658a3c4cbbb04840e3ce7e2c/CqdwhshIL8xxjog_WD_St.png)


### Chat Locally

```bash
lmdeploy chat turbomind ./workspace
```

### TurboMind Inference + API Service

In the previous section, we tried starting the Client directly using the command line. Now, we will attempt to use lmdeploy for service deployment.

The "Model Inference/Service" currently offers two service deployment methods: TurboMind and TritonServer. In this case, the Server is either TurboMind or TritonServer, and the API Server can provide external API services. We recommend using TurboMind.

First, start the service with the following command:


```bash
# ApiServer+Turbomind   api_server => AsyncEngine => TurboMind
lmdeploy serve api_server ./workspace \
	--server_name 0.0.0.0 \
	--server-port 23333 \
	--instance_num 64 \
	--tp 1
```

In the above parameters, `server_name` and `server_port` indicate the service address and port, respectively. The `tp` parameter, as mentioned earlier, stands for Tensor Parallelism. The remaining parameter, instance_num, represents the number of instances and can be understood as the batch size. After execution, it will appear as shown below.

## Web Service Startup Method 1:

###  Starting the Service with Gradio

This section demonstrates using Gradio as a front-end demo.

> Since Gradio requires local access to display the interface,
> you also need to forward the data to your local machine via SSH. The command is as follows:
>
> ssh -CNg -L 6006:127.0.0.1:6006 root@ssh.intern-ai.org.cn -p <your ssh port>

#### --TurboMind Service as the Backend

The API Server is started the same way as in the previous section. Here, we directly start Gradio as the front-end.

```bash
# Gradio+ApiServer. The Server must be started first, and Gradio acts as the Client
lmdeploy serve gradio http://0.0.0.0:23333 --server-port 6006
```

#### --Other way(Recommended!!!)

Of course, Gradio can also connect directly with TurboMind, as shown below

```bash
# Gradio+Turbomind(local)
lmdeploy serve gradio ./workspace
```

You can start Gradio directly. In this case, there is no API Server, and TurboMind communicates directly with Gradio.

## Web Service Startup Method 2:

### Starting the Service with Streamlit

```bash
pip install streamlit==1.24.0
```

Download the [GouMang](https://huggingface.co/AI4Bread/GouMang) project model (please Star if you like it)


Replace the model path in `web_demo.py` with the path where the downloaded parameters of `GouMang` are stored 

Run the `web_demo.py` file in the directory, and after entering the following command, [**check this tutorial 5.2 for local port configuration**](https://github.com/InternLM/tutorial/blob/main/helloworld/hello_world.md#52-%E9%85%8D%E7%BD%AE%E6%9C%AC%E5%9C%B0%E7%AB%AF%E5%8F%A3)ï¼Œto map the port to your local machine. Enter `http://127.0.0.1:6006` in your local browser. 

```
streamlit run /root/personal_assistant/code/InternLM/web_demo.py --server.address 127.0.0.1 --server.port 6006
```

Note: The model will load only after you open the `http://127.0.0.1:6006` page in your browser. 
Once the model is loaded, you can start conversing with GouMang like this.


![image/png](https://cdn-uploads.huggingface.co/production/uploads/658a3c4cbbb04840e3ce7e2c/VcuSpAKrRGY1HP1mwLGI6.png)


## Open Source License

The code is licensed under Apache-2.0, while model weights are fully open for academic research and also allow **free** commercial usage. To apply for a commercial license, please fill in the [ç”³è¯·è¡¨ï¼ˆä¸­æ–‡ï¼‰](https://wj.qq.com/s2/14897739/e871/). For other questions or collaborations, please contact <laiyifu@xjtu.edu.cn>.

## Citation



## ç®€ä»‹

XiXiLM ï¼Œå³è¥¿è¥¿å¤§æ¨¡å‹ï¼ˆåˆåï¼šå¥èŠ’å¤§æ¨¡å‹ï¼‰ï¼Œå¼€æºäº†é¢å‘å†œä¸šé—®ç­”çš„å¤§æ¨¡å‹ã€‚æ¨¡å‹å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

- æœ‰æ•ˆæ”¯æŒ20ä¸‡å­—è¶…é•¿ä¸Šä¸‹æ–‡ï¼šæ¨¡å‹åœ¨20ä¸‡å­—é•¿è¾“å…¥ä¸­å‡ ä¹å®Œç¾åœ°å®ç°é•¿æ–‡â€œå¤§æµ·æé’ˆâ€ï¼Œè€Œä¸”åœ¨ LongBench å’Œ L-Eval ç­‰é•¿æ–‡ä»»åŠ¡ä¸­çš„è¡¨ç°ä¹Ÿè¾¾åˆ°å¼€æºæ¨¡å‹ä¸­çš„é¢†å…ˆæ°´å¹³ã€‚ å¯ä»¥é€šè¿‡ [LMDeploy](https://github.com/InternLM/lmdeploy) å°è¯•20ä¸‡å­—è¶…é•¿ä¸Šä¸‹æ–‡æ¨ç†ã€‚
- ç»¼åˆæ€§èƒ½å…¨é¢æå‡ï¼šå„èƒ½åŠ›ç»´åº¦ç›¸æ¯”ä¸Šä¸€ä»£æ¨¡å‹å…¨é¢è¿›æ­¥ï¼Œåœ¨æ¨ç†ã€æ•°å­¦ã€ä»£ç ã€å¯¹è¯ä½“éªŒã€æŒ‡ä»¤éµå¾ªå’Œåˆ›æ„å†™ä½œç­‰æ–¹é¢çš„èƒ½åŠ›æå‡å°¤ä¸ºæ˜¾è‘—ï¼Œç»¼åˆæ€§èƒ½è¾¾åˆ°åŒé‡çº§å¼€æºæ¨¡å‹çš„é¢†å…ˆæ°´å¹³ï¼Œåœ¨é‡ç‚¹èƒ½åŠ›è¯„æµ‹ä¸Š InternLM2-Chat-20B èƒ½æ¯”è‚©ç”šè‡³è¶…è¶Š ChatGPT ï¼ˆGPT-3.5ï¼‰ã€‚

## XiXiLM-Qwen-14B


**å±€é™æ€§ï¼š** å°½ç®¡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æˆ‘ä»¬éå¸¸æ³¨é‡æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œå°½åŠ›ä¿ƒä½¿æ¨¡å‹è¾“å‡ºç¬¦åˆä¼¦ç†å’Œæ³•å¾‹è¦æ±‚çš„æ–‡æœ¬ï¼Œä½†å—é™äºæ¨¡å‹å¤§å°ä»¥åŠæ¦‚ç‡ç”ŸæˆèŒƒå¼ï¼Œæ¨¡å‹å¯èƒ½ä¼šäº§ç”Ÿå„ç§ä¸ç¬¦åˆé¢„æœŸçš„è¾“å‡ºï¼Œä¾‹å¦‚å›å¤å†…å®¹åŒ…å«åè§ã€æ­§è§†ç­‰æœ‰å®³å†…å®¹ï¼Œè¯·å‹¿ä¼ æ’­è¿™äº›å†…å®¹ã€‚ç”±äºä¼ æ’­ä¸è‰¯ä¿¡æ¯å¯¼è‡´çš„ä»»ä½•åæœï¼Œæœ¬é¡¹ç›®ä¸æ‰¿æ‹…è´£ä»»ã€‚

### é€šè¿‡ Transformers åŠ è½½

é€šè¿‡ä»¥ä¸‹çš„ä»£ç åŠ è½½ InternLM2 7B Chat æ¨¡å‹

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained("AI4Bread/XiXi_Qwen_base_14b", trust_remote_code=True)
# Set `torch_dtype=torch.float16` to load model in float16, otherwise it will be loaded as float32 and cause OOM Error.
model = AutoModelForCausalLM.from_pretrained("AI4Bread/XiXi_Qwen_base_14b", torch_dtype=torch.float16, trust_remote_code=True).cuda()
model = model.eval()
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
# Hello! How can I help you today?
response, history = model.chat(tokenizer, "é©¬é“ƒè–¯è‚²ç§æœ‰ä»€ä¹ˆæ³¨æ„äº‹é¡¹ï¼Ÿéœ€è¦æ³¨æ„ä»€ä¹ˆå‘¢ï¼Ÿ", history=history)
print(response)
```

å¦‚æœæƒ³è¿›è¡Œæµå¼ç”Ÿæˆï¼Œåˆ™å¯ä»¥ä½¿ç”¨ `stream_chat` æ¥å£ï¼š

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = "AI4Bread/XiXi_Qwen_base_14b"
model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, trust_remote_code=True).cuda()
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

model = model.eval()
length = 0
for response, history in model.stream_chat(tokenizer, "é©¬é“ƒè–¯è‚²ç§æœ‰ä»€ä¹ˆæ³¨æ„äº‹é¡¹ï¼Ÿéœ€è¦æ³¨æ„ä»€ä¹ˆå‘¢ï¼Ÿ", history=[]):
    print(response[length:], flush=True, end="")
    length = len(response)
```

## éƒ¨ç½²

### LMDeploy

LMDeploy ç”± MMDeploy å’Œ MMRazor å›¢é˜Ÿè”åˆå¼€å‘ï¼Œæ˜¯æ¶µç›–äº† LLM ä»»åŠ¡çš„å…¨å¥—è½»é‡åŒ–ã€éƒ¨ç½²å’ŒæœåŠ¡è§£å†³æ–¹æ¡ˆã€‚

```bash
pip install lmdeploy
```

ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨å…¼å®¹ OpenAI API çš„æœåŠ¡:

```bash
lmdeploy serve api_server internlm/internlm2-chat-7b --server-port 23333
```

ç„¶åä½ å¯ä»¥å‘æœåŠ¡ç«¯å‘èµ·ä¸€ä¸ªèŠå¤©è¯·æ±‚:

```bash
curl http://localhost:23333/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
    "model": "internlm2-chat-7b",
    "messages": [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†œä¸šä¸“å®¶"},
    {"role": "user", "content": "é©¬é“ƒè–¯ç§æ¤çš„æ—¶å€™æœ‰å“ªäº›æ³¨æ„äº‹é¡¹ï¼Ÿ"}
    ]
    }'
```

æ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹ [LMDeploy æ–‡æ¡£](https://lmdeploy.readthedocs.io/en/latest/)

### vLLM

ä½¿ç”¨`vLLM>=0.3.2`å¯åŠ¨å…¼å®¹ OpenAI API çš„æœåŠ¡:

```bash
pip install vllm
```

```bash
python -m vllm.entrypoints.openai.api_server --model internlm/internlm2-chat-7b --trust-remote-code
```

ç„¶åä½ å¯ä»¥å‘æœåŠ¡ç«¯å‘èµ·ä¸€ä¸ªèŠå¤©è¯·æ±‚:

```bash
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
    "model": "internlm2-chat-7b",
    "messages": [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†œä¸šä¸“å®¶."},
    {"role": "user", "content": "è¯·ç»™æˆ‘ä»‹ç»ä¸€ä¸‹é©¬é“ƒè–¯è‚²ç§."}
    ]
    }'
```

æ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹ [vLLM æ–‡æ¡£](https://docs.vllm.ai/en/latest/index.html)


## ç½‘é¡µæœåŠ¡å¯åŠ¨æ–¹å¼1:

###  Gradio æ–¹å¼å¯åŠ¨æœåŠ¡

è¿™ä¸€éƒ¨åˆ†ä¸»è¦æ˜¯å°† Gradio ä½œä¸ºå‰ç«¯ Demo æ¼”ç¤ºã€‚åœ¨ä¸Šä¸€èŠ‚çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ä¸æ‰§è¡Œåé¢çš„ `api_client` æˆ– `triton_client`ï¼Œè€Œæ˜¯æ‰§è¡Œ `gradio`ã€‚
è¯·å‚è€ƒ[LMDeploy](#lmdeploy)éƒ¨åˆ†è·å–è¯¦ç»†ä¿¡æ¯ã€‚

> ç”±äº Gradio éœ€è¦æœ¬åœ°è®¿é—®å±•ç¤ºç•Œé¢ï¼Œå› æ­¤ä¹Ÿéœ€è¦é€šè¿‡ ssh å°†æ•°æ®è½¬å‘åˆ°æœ¬åœ°ã€‚å‘½ä»¤å¦‚ä¸‹ï¼š
>
> ssh -CNg -L 6006:127.0.0.1:6006 root@ssh.intern-ai.org.cn -p <ä½ çš„ ssh ç«¯å£å·>

#### --TurboMind æœåŠ¡ä½œä¸ºåç«¯

ç›´æ¥å¯åŠ¨ä½œä¸ºå‰ç«¯çš„ Gradioã€‚

```bash
# Gradio+ApiServerã€‚å¿…é¡»å…ˆå¼€å¯ Serverï¼Œæ­¤æ—¶ Gradio ä¸º Client
lmdeploy serve gradio http://0.0.0.0:23333 --server-port 6006
```

#### --å…¶ä»–æ–¹å¼(æ¨è!!!)

å½“ç„¶ï¼ŒGradio ä¹Ÿå¯ä»¥ç›´æ¥å’Œ TurboMind è¿æ¥ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

```bash
# Gradio+Turbomind(local)
lmdeploy serve gradio ./workspace
```

å¯ä»¥ç›´æ¥å¯åŠ¨ Gradioï¼Œæ­¤æ—¶æ²¡æœ‰ API Serverï¼ŒTurboMind ç›´æ¥ä¸ Gradio é€šä¿¡ã€‚

## ç½‘é¡µæœåŠ¡å¯åŠ¨æ–¹å¼2:

### Streamlit æ–¹å¼å¯åŠ¨æœåŠ¡ï¼š

ä¸‹è½½ [GouMang](https://huggingface.co/AI4Bread/GouMang) é¡¹ç›®æ¨¡å‹ï¼ˆå¦‚æœå–œæ¬¢è¯·ç»™ä¸ª Starï¼‰

å°† `web_demo.py` ä¸­çš„æ¨¡å‹è·¯å¾„æ›¿æ¢ä¸ºä¸‹è½½çš„ `GouMang` å‚æ•°å­˜å‚¨è·¯å¾„

åœ¨ç›®å½•ä¸­è¿è¡Œ `web_demo.py` æ–‡ä»¶ï¼Œå¹¶åœ¨è¾“å…¥ä»¥ä¸‹å‘½ä»¤åï¼Œ[**æŸ¥çœ‹æœ¬æ•™ç¨‹ 5.2 ä»¥é…ç½®æœ¬åœ°ç«¯å£**](https://github.com/InternLM/tutorial/blob/main/helloworld/hello_world.md#52-%E9%85%8D%E7%BD%AE%E6%9C%AC%E5%9C%B0%E7%AB%AF%E5%8F%A3)ï¼Œå°†ç«¯å£æ˜ å°„åˆ°æœ¬åœ°ã€‚åœ¨æœ¬åœ°æµè§ˆå™¨ä¸­è¾“å…¥ `http://127.0.0.1:6006`ã€‚

```
streamlit run /root/personal_assistant/code/InternLM/web_demo.py --server.address 127.0.0.1 --server.port 6006
```

æ³¨æ„ï¼šåªæœ‰åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ `http://127.0.0.1:6006` é¡µé¢åï¼Œæ¨¡å‹æ‰ä¼šåŠ è½½ã€‚
æ¨¡å‹åŠ è½½å®Œæˆåï¼Œæ‚¨å°±å¯ä»¥å¼€å§‹ä¸ è¥¿è¥¿ï¼ˆå¥èŠ’ï¼‰ è¿›è¡Œå¯¹è¯äº†ã€‚

## å¼€æºè®¸å¯è¯

æœ¬ä»“åº“çš„ä»£ç ä¾ç…§ Apache-2.0 åè®®å¼€æºã€‚æ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œä¹Ÿå¯ç”³è¯·å…è´¹çš„å•†ä¸šä½¿ç”¨æˆæƒï¼ˆ[ç”³è¯·è¡¨](https://wj.qq.com/s2/14897739/e871/)ï¼‰ã€‚å…¶ä»–é—®é¢˜ä¸åˆä½œè¯·è”ç³» <laiyifu@xjtu.edu.cn>ã€‚

## å¼•ç”¨
