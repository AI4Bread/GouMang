# XiXiLM 

<div align="center">

<img src="https://github.com/AI4Bread/GouMang/blob/main/assets/goumang_logoallnew.png?raw=true" width="600"/>
  <div>&nbsp;</div>
  <div align="center">
    <!-- <b><font size="5">XiXiLM</font></b> -->
    <sup>
      <a href="http://www.ai4bread.com">
      </a>
    </sup>
    <div>&nbsp;</div>
  </div>
  

[ğŸ’»Github Repo](https://github.com/AI4Bread/GouMang) â€¢ [ğŸ¤”Reporting Issues](https://github.com/AI4Bread/GouMang/issues) â€¢ [ğŸ“œTechnical Report](https://github.com/AI4Bread)

</div>

<p align="center">
    ğŸ‘‹ join us on <a href="https://github.com/AI4Bread/GouMang" target="_blank">Github</a>
</p>



## Introduction

XiXiLMï¼ˆGouMang LLMï¼‰ has open-sourced a 7 billion parameter base model and a chat model tailored for agricultural scenarios. The model has the following characteristics:

1. **High Professionalism**: XiXiLM focuses on the agricultural field, providing professional and accurate answers especially in areas such as tuber crop cultivation, pest and disease control, and soil management.

2. **Academic Support**: The model is based on the latest agricultural research findings, capable of providing academic-level answers to help researchers and agricultural practitioners gain a deeper understanding of agricultural issues.

3. **Multilingual Support**: Supports both Chinese and English languages, making it convenient for users both domestically and internationally.

4. **Free Commercial Use**: The model weights are fully open, supporting not only academic research but also allowing **free** commercial usage. Users can use the model in commercial projects for free, lowering the usage threshold.

5. **Efficient Training**: Employs advanced training algorithms and techniques, enabling the model to respond quickly to user inquiries and provide efficient Q&A services.

6. **Continuous Optimization**: The model will be continuously optimized based on user feedback and the latest research findings, constantly improving the quality and coverage of its answers.

## XiXiLM-Qwen-14B


**Limitations:** Although we have made efforts to ensure the safety of the model during the training process and to 
encourage the model to generate text that complies with ethical and legal requirements, the model may still produce unexpected 
outputs due to its size and probabilistic generation paradigm. For example, the generated responses may contain biases, discrimination, 
or other harmful content. Please do not propagate such content. We are not responsible for any consequences resulting from the 
dissemination of harmful information.

### Import from Transformers

To load the XiXiLM model using Transformers, use the following code:

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained("AI4Bread/XiXi_Qwen_base_14b", trust_remote_code=True)
# Set `torch_dtype=torch.float16` to load model in float16, otherwise it will be loaded as float32 and cause OOM Error.
model = AutoModelForCausalLM.from_pretrained("AI4Bread/XiXi_Qwen_base_14b", torch_dtype=torch.float16, trust_remote_code=True).cuda()
model = model.eval()
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
# Hello! How can I help you today?
response, history = model.chat(tokenizer, "é©¬é“ƒè–¯è‚²ç§æœ‰ä»€ä¹ˆæ³¨æ„äº‹é¡¹ï¼Ÿéœ€è¦æ³¨æ„ä»€ä¹ˆå‘¢ï¼Ÿ", history=history)
print(response)
```

The responses can be streamed using `stream_chat`:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = "AI4Bread/XiXi_Qwen_base_14b"
model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, trust_remote_code=True).cuda()
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

model = model.eval()
length = 0
for response, history in model.stream_chat(tokenizer, "Hello", history=[]):
    print(response[length:], flush=True, end="")
    length = len(response)
```


## Deployment

### LMDeploy

LMDeploy is a toolkit for compressing, deploying, and serving LLM, developed by the MMRazor and MMDeploy teams.

```bash
pip install lmdeploy
```

Or you can launch an OpenAI compatible server with the following command:

```bash
lmdeploy serve api_server internlm/internlm2-chat-7b --model-name internlm2-chat-7b --server-port 23333 
```

Then you can send a chat request to the server:

```bash
curl http://localhost:23333/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
    "model": "internlm2-chat-7b",
    "messages": [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†œä¸šä¸“å®¶"},
    {"role": "user", "content": "é©¬é“ƒè–¯ç§æ¤çš„æ—¶å€™æœ‰å“ªäº›æ³¨æ„äº‹é¡¹ï¼Ÿ"}
    ]
    }'
```

The output be like:

![image/png](https://cdn-uploads.huggingface.co/production/uploads/658a3c4cbbb04840e3ce7e2c/NPdRr5Y5l5E0m0URCVZ1f.png)



Find more details in the [LMDeploy documentation](https://lmdeploy.readthedocs.io/en/latest/)

### vLLM

Launch OpenAI compatible server with `vLLM>=0.3.2`:

```bash
pip install vllm
```

```bash
python -m vllm.entrypoints.openai.api_server --model internlm/internlm2-chat-7b --served-model-name internlm2-chat-7b --trust-remote-code
```

Then you can send a chat request to the server:

```bash
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
    "model": "internlm2-chat-7b",
    "messages": [
    {"role": "system", "content": "You are a professional agriculture expert."},
    {"role": "user", "content": "Introduce potato farming to me."}
    ]
    }'
```

Find more details in the [vLLM documentation](https://docs.vllm.ai/en/latest/index.html)

### Convert  lmdeploy TurboMind

```bash
# Converting Model to TurboMind (FastTransformer Format)
lmdeploy convert internlm-chat-7b /path/to/XiXiLM
```

Here, we will use our pre-trained model file and execute the conversion in the user's root directory, as shown below.

```bash
lmdeploy convert internlm2-chat-7b /root/autodl-tmp/agri_intern/XiXiLM --tokenizer-path ./GouMang/tokenizer.json
```

After execution, a workspace folder will be generated in the current directory. 
This folder contains the necessary files for TurboMind and Triton "Model Inference." as shown below:


![image/png](https://cdn-uploads.huggingface.co/production/uploads/658a3c4cbbb04840e3ce7e2c/CqdwhshIL8xxjog_WD_St.png)


### Chat Locally

```bash
lmdeploy chat turbomind ./workspace
```

### TurboMind Inference + API Service

In the previous section, we tried starting the Client directly using the command line. Now, we will attempt to use lmdeploy for service deployment.

The "Model Inference/Service" currently offers two service deployment methods: TurboMind and TritonServer. In this case, the Server is either TurboMind or TritonServer, and the API Server can provide external API services. We recommend using TurboMind.

First, start the service with the following command:


```bash
# ApiServer+Turbomind   api_server => AsyncEngine => TurboMind
lmdeploy serve api_server ./workspace \
	--server_name 0.0.0.0 \
	--server-port 23333 \
	--instance_num 64 \
	--tp 1
```

In the above parameters, `server_name` and `server_port` indicate the service address and port, respectively. The `tp` parameter, as mentioned earlier, stands for Tensor Parallelism. The remaining parameter, instance_num, represents the number of instances and can be understood as the batch size. After execution, it will appear as shown below.

## Web Service Startup Method 1:

###  Starting the Service with Gradio

This section demonstrates using Gradio as a front-end demo.

> Since Gradio requires local access to display the interface,
> you also need to forward the data to your local machine via SSH. The command is as follows:
>
> ssh -CNg -L 6006:127.0.0.1:6006 root@ssh.intern-ai.org.cn -p <your ssh port>

#### --TurboMind Service as the Backend

The API Server is started the same way as in the previous section. Here, we directly start Gradio as the front-end.

```bash
# Gradio+ApiServer. The Server must be started first, and Gradio acts as the Client
lmdeploy serve gradio http://0.0.0.0:23333 --server-port 6006
```

#### --Other way(Recommended!!!)

Of course, Gradio can also connect directly with TurboMind, as shown below

```bash
# Gradio+Turbomind(local)
lmdeploy serve gradio ./workspace
```

You can start Gradio directly. In this case, there is no API Server, and TurboMind communicates directly with Gradio.

## Web Service Startup Method 2:

### Starting the Service with Streamlit

```bash
pip install streamlit==1.24.0
```

Download the [GouMang](https://huggingface.co/AI4Bread/GouMang) project model (please Star if you like it)


Replace the model path in `web_demo.py` with the path where the downloaded parameters of `GouMang` are stored 

Run the `web_demo.py` file in the directory, and after entering the following command, [**check this tutorial 5.2 for local port configuration**](https://github.com/InternLM/tutorial/blob/main/helloworld/hello_world.md#52-%E9%85%8D%E7%BD%AE%E6%9C%AC%E5%9C%B0%E7%AB%AF%E5%8F%A3)ï¼Œto map the port to your local machine. Enter `http://127.0.0.1:6006` in your local browser. 

```
streamlit run /root/personal_assistant/code/InternLM/web_demo.py --server.address 127.0.0.1 --server.port 6006
```

Note: The model will load only after you open the `http://127.0.0.1:6006` page in your browser. 
Once the model is loaded, you can start conversing with GouMang like this.


![image/png](https://cdn-uploads.huggingface.co/production/uploads/658a3c4cbbb04840e3ce7e2c/VcuSpAKrRGY1HP1mwLGI6.png)


## Open Source License

The code is licensed under Apache-2.0, while model weights are fully open for academic research and also allow **free** commercial usage. To apply for a commercial license, please fill in the <a href="https://wj.qq.com/s2/14897739/e871/" target="_blank">ç”³è¯·è¡¨ï¼ˆä¸­æ–‡ï¼‰</a>. For other questions or collaborations, please contact <laiyifu@xjtu.edu.cn>.

## Citation



## ç®€ä»‹

XiXiLM ï¼Œå³è¥¿è¥¿å¤§æ¨¡å‹ï¼ˆåˆåï¼šå¥èŠ’å¤§æ¨¡å‹ï¼‰ï¼Œå¼€æºäº†é¢å‘å†œä¸šé—®ç­”çš„å¤§æ¨¡å‹ã€‚æ¨¡å‹å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. **ä¸“ä¸šæ€§å¼º**ï¼šXiXiLM ä¸“æ³¨äºå†œä¸šé¢†åŸŸï¼Œç‰¹åˆ«æ˜¯è–¯ç±»ä½œç‰©çš„ç§æ¤ã€ç—…è™«å®³é˜²æ²»ã€åœŸå£¤ç®¡ç†ç­‰æ–¹é¢ï¼Œæä¾›ä¸“ä¸šã€ç²¾å‡†çš„è§£ç­”ã€‚

2. **å­¦æœ¯åŒ–æ”¯æŒ**ï¼šæ¨¡å‹åŸºäºæœ€æ–°çš„å†œä¸šç ”ç©¶æˆæœï¼Œèƒ½å¤Ÿæä¾›å­¦æœ¯åŒ–çš„å›ç­”ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå†œä¸šä»ä¸šè€…æ·±å…¥ç†è§£å†œä¸šé—®é¢˜ã€‚

3. **å¤šè¯­è¨€æ”¯æŒ**ï¼šæ”¯æŒä¸­æ–‡å’Œè‹±æ–‡ä¸¤ç§è¯­è¨€ï¼Œæ–¹ä¾¿å›½å†…å¤–ç”¨æˆ·ä½¿ç”¨ã€‚

4. **å…è´¹å•†ä¸šä½¿ç”¨**ï¼šæ¨¡å‹æƒé‡å®Œå…¨å¼€æ”¾ï¼Œä¸ä»…æ”¯æŒå­¦æœ¯ç ”ç©¶ï¼Œè¿˜å…è®¸**ç”³è¯·**å•†ä¸šä½¿ç”¨ã€‚ç”¨æˆ·å¯ä»¥åœ¨å•†ä¸šé¡¹ç›®ä¸­å…è´¹ä½¿ç”¨è¯¥æ¨¡å‹ï¼Œé™ä½äº†ä½¿ç”¨é—¨æ§›ã€‚

5. **é«˜æ•ˆè®­ç»ƒ**ï¼šé‡‡ç”¨å…ˆè¿›çš„è®­ç»ƒç®—æ³•å’ŒæŠ€æœ¯ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå¿«é€Ÿå“åº”ç”¨æˆ·æé—®ï¼Œæä¾›é«˜æ•ˆçš„é—®ç­”æœåŠ¡ã€‚

6. **æŒç»­ä¼˜åŒ–**ï¼šæ¨¡å‹ä¼šæ ¹æ®ç”¨æˆ·åé¦ˆå’Œæœ€æ–°ç ”ç©¶æˆæœè¿›è¡ŒæŒç»­ä¼˜åŒ–ï¼Œä¸æ–­æå‡é—®ç­”è´¨é‡å’Œè¦†ç›–é¢ã€‚


## XiXiLM-Qwen-14B


**å±€é™æ€§ï¼š** å°½ç®¡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æˆ‘ä»¬éå¸¸æ³¨é‡æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œå°½åŠ›ä¿ƒä½¿æ¨¡å‹è¾“å‡ºç¬¦åˆä¼¦ç†å’Œæ³•å¾‹è¦æ±‚çš„æ–‡æœ¬ï¼Œä½†å—é™äºæ¨¡å‹å¤§å°ä»¥åŠæ¦‚ç‡ç”ŸæˆèŒƒå¼ï¼Œæ¨¡å‹å¯èƒ½ä¼šäº§ç”Ÿå„ç§ä¸ç¬¦åˆé¢„æœŸçš„è¾“å‡ºï¼Œä¾‹å¦‚å›å¤å†…å®¹åŒ…å«åè§ã€æ­§è§†ç­‰æœ‰å®³å†…å®¹ï¼Œè¯·å‹¿ä¼ æ’­è¿™äº›å†…å®¹ã€‚ç”±äºä¼ æ’­ä¸è‰¯ä¿¡æ¯å¯¼è‡´çš„ä»»ä½•åæœï¼Œæœ¬é¡¹ç›®ä¸æ‰¿æ‹…è´£ä»»ã€‚

### é€šè¿‡ Transformers åŠ è½½

é€šè¿‡ä»¥ä¸‹çš„ä»£ç åŠ è½½ InternLM2 7B Chat æ¨¡å‹

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained("AI4Bread/XiXi_Qwen_base_14b", trust_remote_code=True)
# Set `torch_dtype=torch.float16` to load model in float16, otherwise it will be loaded as float32 and cause OOM Error.
model = AutoModelForCausalLM.from_pretrained("AI4Bread/XiXi_Qwen_base_14b", torch_dtype=torch.float16, trust_remote_code=True).cuda()
model = model.eval()
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
# Hello! How can I help you today?
response, history = model.chat(tokenizer, "é©¬é“ƒè–¯è‚²ç§æœ‰ä»€ä¹ˆæ³¨æ„äº‹é¡¹ï¼Ÿéœ€è¦æ³¨æ„ä»€ä¹ˆå‘¢ï¼Ÿ", history=history)
print(response)
```

å¦‚æœæƒ³è¿›è¡Œæµå¼ç”Ÿæˆï¼Œåˆ™å¯ä»¥ä½¿ç”¨ `stream_chat` æ¥å£ï¼š

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = "AI4Bread/XiXi_Qwen_base_14b"
model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, trust_remote_code=True).cuda()
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

model = model.eval()
length = 0
for response, history in model.stream_chat(tokenizer, "é©¬é“ƒè–¯è‚²ç§æœ‰ä»€ä¹ˆæ³¨æ„äº‹é¡¹ï¼Ÿéœ€è¦æ³¨æ„ä»€ä¹ˆå‘¢ï¼Ÿ", history=[]):
    print(response[length:], flush=True, end="")
    length = len(response)
```

## éƒ¨ç½²

### LMDeploy

LMDeploy ç”± MMDeploy å’Œ MMRazor å›¢é˜Ÿè”åˆå¼€å‘ï¼Œæ˜¯æ¶µç›–äº† LLM ä»»åŠ¡çš„å…¨å¥—è½»é‡åŒ–ã€éƒ¨ç½²å’ŒæœåŠ¡è§£å†³æ–¹æ¡ˆã€‚

```bash
pip install lmdeploy
```

ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨å…¼å®¹ OpenAI API çš„æœåŠ¡:

```bash
lmdeploy serve api_server internlm/internlm2-chat-7b --server-port 23333
```

ç„¶åä½ å¯ä»¥å‘æœåŠ¡ç«¯å‘èµ·ä¸€ä¸ªèŠå¤©è¯·æ±‚:

```bash
curl http://localhost:23333/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
    "model": "internlm2-chat-7b",
    "messages": [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†œä¸šä¸“å®¶"},
    {"role": "user", "content": "é©¬é“ƒè–¯ç§æ¤çš„æ—¶å€™æœ‰å“ªäº›æ³¨æ„äº‹é¡¹ï¼Ÿ"}
    ]
    }'
```

æ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹ [LMDeploy æ–‡æ¡£](https://lmdeploy.readthedocs.io/en/latest/)

### vLLM

ä½¿ç”¨`vLLM>=0.3.2`å¯åŠ¨å…¼å®¹ OpenAI API çš„æœåŠ¡:

```bash
pip install vllm
```

```bash
python -m vllm.entrypoints.openai.api_server --model internlm/internlm2-chat-7b --trust-remote-code
```

ç„¶åä½ å¯ä»¥å‘æœåŠ¡ç«¯å‘èµ·ä¸€ä¸ªèŠå¤©è¯·æ±‚:

```bash
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
    "model": "internlm2-chat-7b",
    "messages": [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†œä¸šä¸“å®¶."},
    {"role": "user", "content": "è¯·ç»™æˆ‘ä»‹ç»ä¸€ä¸‹é©¬é“ƒè–¯è‚²ç§."}
    ]
    }'
```

æ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹ [vLLM æ–‡æ¡£](https://docs.vllm.ai/en/latest/index.html)


## ç½‘é¡µæœåŠ¡å¯åŠ¨æ–¹å¼1:

###  Gradio æ–¹å¼å¯åŠ¨æœåŠ¡

è¿™ä¸€éƒ¨åˆ†ä¸»è¦æ˜¯å°† Gradio ä½œä¸ºå‰ç«¯ Demo æ¼”ç¤ºã€‚åœ¨ä¸Šä¸€èŠ‚çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ä¸æ‰§è¡Œåé¢çš„ `api_client` æˆ– `triton_client`ï¼Œè€Œæ˜¯æ‰§è¡Œ `gradio`ã€‚
è¯·å‚è€ƒ[LMDeploy](#lmdeploy)éƒ¨åˆ†è·å–è¯¦ç»†ä¿¡æ¯ã€‚

> ç”±äº Gradio éœ€è¦æœ¬åœ°è®¿é—®å±•ç¤ºç•Œé¢ï¼Œå› æ­¤ä¹Ÿéœ€è¦é€šè¿‡ ssh å°†æ•°æ®è½¬å‘åˆ°æœ¬åœ°ã€‚å‘½ä»¤å¦‚ä¸‹ï¼š
>
> ssh -CNg -L 6006:127.0.0.1:6006 root@ssh.intern-ai.org.cn -p <ä½ çš„ ssh ç«¯å£å·>

#### --TurboMind æœåŠ¡ä½œä¸ºåç«¯

ç›´æ¥å¯åŠ¨ä½œä¸ºå‰ç«¯çš„ Gradioã€‚

```bash
# Gradio+ApiServerã€‚å¿…é¡»å…ˆå¼€å¯ Serverï¼Œæ­¤æ—¶ Gradio ä¸º Client
lmdeploy serve gradio http://0.0.0.0:23333 --server-port 6006
```

#### --å…¶ä»–æ–¹å¼(æ¨è!!!)

å½“ç„¶ï¼ŒGradio ä¹Ÿå¯ä»¥ç›´æ¥å’Œ TurboMind è¿æ¥ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

```bash
# Gradio+Turbomind(local)
lmdeploy serve gradio ./workspace
```

å¯ä»¥ç›´æ¥å¯åŠ¨ Gradioï¼Œæ­¤æ—¶æ²¡æœ‰ API Serverï¼ŒTurboMind ç›´æ¥ä¸ Gradio é€šä¿¡ã€‚

## ç½‘é¡µæœåŠ¡å¯åŠ¨æ–¹å¼2:

### Streamlit æ–¹å¼å¯åŠ¨æœåŠ¡ï¼š

ä¸‹è½½ [GouMang](https://huggingface.co/AI4Bread/GouMang) é¡¹ç›®æ¨¡å‹ï¼ˆå¦‚æœå–œæ¬¢è¯·ç»™ä¸ª Starï¼‰

å°† `web_demo.py` ä¸­çš„æ¨¡å‹è·¯å¾„æ›¿æ¢ä¸ºä¸‹è½½çš„ `GouMang` å‚æ•°å­˜å‚¨è·¯å¾„

åœ¨ç›®å½•ä¸­è¿è¡Œ `web_demo.py` æ–‡ä»¶ï¼Œå¹¶åœ¨è¾“å…¥ä»¥ä¸‹å‘½ä»¤åï¼Œ[**æŸ¥çœ‹æœ¬æ•™ç¨‹ 5.2 ä»¥é…ç½®æœ¬åœ°ç«¯å£**](https://github.com/InternLM/tutorial/blob/main/helloworld/hello_world.md#52-%E9%85%8D%E7%BD%AE%E6%9C%AC%E5%9C%B0%E7%AB%AF%E5%8F%A3)ï¼Œå°†ç«¯å£æ˜ å°„åˆ°æœ¬åœ°ã€‚åœ¨æœ¬åœ°æµè§ˆå™¨ä¸­è¾“å…¥ `http://127.0.0.1:6006`ã€‚

```
streamlit run /root/personal_assistant/code/InternLM/web_demo.py --server.address 127.0.0.1 --server.port 6006
```

æ³¨æ„ï¼šåªæœ‰åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ `http://127.0.0.1:6006` é¡µé¢åï¼Œæ¨¡å‹æ‰ä¼šåŠ è½½ã€‚
æ¨¡å‹åŠ è½½å®Œæˆåï¼Œæ‚¨å°±å¯ä»¥å¼€å§‹ä¸ è¥¿è¥¿ï¼ˆå¥èŠ’ï¼‰ è¿›è¡Œå¯¹è¯äº†ã€‚

## å¼€æºè®¸å¯è¯

æœ¬ä»“åº“çš„ä»£ç ä¾ç…§ Apache-2.0 åè®®å¼€æºã€‚æ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œä¹Ÿå¯ç”³è¯·å…è´¹çš„å•†ä¸šä½¿ç”¨æˆæƒï¼ˆ<a href="https://wj.qq.com/s2/14897739/e871/" target="_blank">ç”³è¯·è¡¨ï¼ˆä¸­æ–‡ï¼‰</a>ï¼‰ã€‚å…¶ä»–é—®é¢˜ä¸åˆä½œè¯·è”ç³» <laiyifu@xjtu.edu.cn>ã€‚

## å¼•ç”¨
